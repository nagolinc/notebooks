{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964ccced",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from shap_e.diffusion.sample import sample_latents\n",
    "from shap_e.diffusion.gaussian_diffusion import diffusion_from_config\n",
    "from shap_e.models.download import load_model, load_config\n",
    "from shap_e.util.notebooks import create_pan_cameras, decode_latent_images, gif_widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eed3a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d922637",
   "metadata": {},
   "outputs": [],
   "source": [
    "xm = load_model('transmitter', device=device)\n",
    "model = load_model('text300M', device=device)\n",
    "diffusion = diffusion_from_config(load_config('diffusion'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d329d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "guidance_scale = 15.0\n",
    "prompt = \"taylor swift in a red dress\"\n",
    "\n",
    "latents = sample_latents(\n",
    "    batch_size=batch_size,\n",
    "    model=model,\n",
    "    diffusion=diffusion,\n",
    "    guidance_scale=guidance_scale,\n",
    "    model_kwargs=dict(texts=[prompt] * batch_size),\n",
    "    progress=True,\n",
    "    clip_denoised=True,\n",
    "    use_fp16=True,\n",
    "    use_karras=True,\n",
    "    karras_steps=64,\n",
    "    sigma_min=1e-3,\n",
    "    sigma_max=160,\n",
    "    s_churn=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633da2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "render_mode = 'nerf' # you can change this to 'stf'\n",
    "size = 64 # this is the size of the renders; higher values take longer to render.\n",
    "\n",
    "cameras = create_pan_cameras(size, device)\n",
    "for i, latent in enumerate(latents):\n",
    "    images = decode_latent_images(xm, latent, cameras, rendering_mode=render_mode)\n",
    "    display(gif_widget(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04626a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "render_mode = 'stf' # you can change this to 'stf'\n",
    "size = 64 # this is the size of the renders; higher values take longer to render.\n",
    "\n",
    "cameras = create_pan_cameras(size, device)\n",
    "for i, latent in enumerate(latents):\n",
    "    images = decode_latent_images(xm, latent, cameras, rendering_mode=render_mode)\n",
    "    display(gif_widget(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c73260",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0095990",
   "metadata": {},
   "outputs": [],
   "source": [
    "rendering_mode='stf'\n",
    "size=64\n",
    "cameras = create_pan_cameras(size, device)\n",
    "latent=latents[0]\n",
    "from shap_e.models.transmitter.base import Transmitter, VectorDecoder\n",
    "from shap_e.util.collections import AttrDict\n",
    "decoded = xm.renderer.render_views(\n",
    "        AttrDict(cameras=cameras),\n",
    "        params=(xm.encoder if isinstance(xm, Transmitter) else xm).bottleneck_to_params(\n",
    "            latent[None]\n",
    "        ),\n",
    "        options=AttrDict(rendering_mode=rendering_mode, render_with_direction=False),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acce4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import io\n",
    "from typing import Union\n",
    "\n",
    "import ipywidgets as widgets\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "from shap_e.models.nn.camera import DifferentiableCameraBatch, DifferentiableProjectiveCamera\n",
    "from shap_e.models.transmitter.base import Transmitter, VectorDecoder\n",
    "from shap_e.util.collections import AttrDict\n",
    "\n",
    "@torch.no_grad()\n",
    "def decode_latent_images_foo(\n",
    "    xm: Union[Transmitter, VectorDecoder],\n",
    "    latent: torch.Tensor,\n",
    "    cameras: DifferentiableCameraBatch,\n",
    "    rendering_mode: str = \"stf\",\n",
    "):\n",
    "    decoded = xm.renderer.render_views(\n",
    "        AttrDict(cameras=cameras),\n",
    "        params=(xm.encoder if isinstance(xm, Transmitter) else xm).bottleneck_to_params(\n",
    "            latent[None]\n",
    "        ),\n",
    "        options=AttrDict(rendering_mode=rendering_mode, render_with_direction=False),\n",
    "    )\n",
    "    return decoded\n",
    "    arr = decoded.channels.clamp(0, 255).to(torch.uint8)[0].cpu().numpy()\n",
    "    return [Image.fromarray(x) for x in arr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306155cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=decode_latent_images_foo(xm, latents[0], cameras, rendering_mode=render_mode)\n",
    "#x['meshes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c021600",
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh=x['meshes'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ebb6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rm=x['raw_meshes'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c35079c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rm.vertex_channels[\"R\"]=mesh.vertex_colors[:,0]\n",
    "rm.vertex_channels[\"G\"]=mesh.vertex_colors[:,1]\n",
    "rm.vertex_channels[\"B\"]=mesh.vertex_colors[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844d5e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "tm=rm.tri_mesh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8618992",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"yoda.ply\",'wb') as f:\n",
    "    tm.write_ply(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb1dee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import trimesh\n",
    "\n",
    "def convert_ply_to_gltf(ply_file, gltf_file):\n",
    "    # Load the .ply file\n",
    "    mesh = trimesh.load_mesh(ply_file)\n",
    "\n",
    "    # Export the mesh to .gltf format\n",
    "    gltf_data = mesh.export(file_type='glb')\n",
    "\n",
    "    # Write the .gltf file\n",
    "    with open(gltf_file, 'wb') as f:\n",
    "        f.write(gltf_data)\n",
    "\n",
    "# Replace these with your input and output file paths\n",
    "input_ply_file = \"yoda.ply\"\n",
    "output_gltf_file = \"yoda.glb\"\n",
    "\n",
    "# Convert the .ply file to .gltf\n",
    "convert_ply_to_gltf(input_ply_file, output_gltf_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8367c0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98a3454",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
